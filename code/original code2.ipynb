{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "f = open(os.path.join(os.getcwd(),\"FA00039662013-06-12.txt\"),\"r\").readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def better_Tokenizer(doc):\n",
    "    txt2 = re.sub(r',|\\.|\\?|<br|/>|\\:|;|\\(|\\)',\"\",doc)\n",
    "    txt3 = re.sub(r'\\'ll',\" will\",txt2)\n",
    "    txt4 = re.sub(r'\\'m',\" am\",txt3)\n",
    "    txt5 = re.sub(r'\\'s',\" is\",txt4)\n",
    "    lowered_tokens = txt5.lower()\n",
    "    return lowered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "f = open(os.path.join(os.getcwd(),\"fulldata.txt\"),\"r\").readlines()\n",
    "print(len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126245\n",
      "chapala is is simply put the mexican restaurant with the best food the best staff and the best margaritas in town their grandpa margarita has been a staple since i've turned 21 their waiters are always friendly and have not changed for years they love their jobs and their customers and it really comes through in the service\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(f1))\n",
    "print(f1[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "POS_LABEL = 'pos'\n",
    "NEG_LABEL = 'neg'\n",
    "vocab = set()\n",
    "posVocab = set()\n",
    "negVocab = set()\n",
    "def tokenize_doc(doc):\n",
    "    \"\"\"\n",
    "\n",
    "    Tokenize a document and return its bag-of-words representation.\n",
    "    doc - a string representing a document.\n",
    "    returns a dictionary mapping each word to the number of times it appears in doc.\n",
    "    \"\"\"\n",
    "    bow = defaultdict(float)\n",
    "    tokens = doc.split()\n",
    "    lowered_tokens = map(lambda t: t.lower(), tokens)\n",
    "    for token in lowered_tokens:\n",
    "        #print(token)\n",
    "        bow[token] += 1.0\n",
    "    return bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_model(bow, label):\n",
    "    for k in bow:\n",
    "        vocab.add(k)\n",
    "        if label == POS_LABEL:\n",
    "            posVocab.add(k)\n",
    "        elif label == NEG_LABEL:\n",
    "            negVocab.add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(),\"FA00039662013-06-12.txt\"),\"r\",encoding='latin1') as doc:\n",
    "    content = doc.read()\n",
    "    bow = tokenize_doc(content)\n",
    "    update_model(bow, NEG_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(my', 'size', 'thursdays', 'unless', 'one,', 'tropicana', 'fat', 'heavily', 'those', 'sneak', 'out', 'cfa).', 'list', 'strong', 'over*priced', \"up'\", 'paid', 'was', 'and', 'make', 'minutes', 'slowed', '(great', 'me', 'night', 'staff', 'turkey', \"son's\", 'instead,', 'guythis', 'forgot', 'chicken--ever!get', 'w.', 'old.i', 'today,', 'joke.', 'instead', 'keeping', 'greasy', 'bones.', 'forget', 'steve', 'are', 'kid', 'knife', 'hands', 'same', 'fingers', 'served', 'terrible!', 'went', 'come', 'eat', 'kids', 'half', 'else', 'than', 'bobbie,', 'all', 'until', 'crunchy', 'get', 'any-type', 'into', \"'dem\", 'delights', 'cholesterol', 'did', 'already', 'had)', 'snacker', 'close', 'in', 'hahaa!', 'try', 'times', \"'grew\", 'heard', '\"we\\'re', 'themselves', 'way.', 'wash', '-', 'cleanest', 'rekindle', 'swill', 'order', 'reviewing,', 'sad', 'top', 'animal', 'kid.', \"he's\", 'world', 'told', 'more', '6', 'getting', 'french', 'food,', 'sure.', 'trade', 'anything', 'dilutional', \"i'm\", 'that', 'other', 'gravey,', 'not!!!', 'filthy!', 'restaurant', 'be', 'trying', 'wife', '$9', 'will', 'table.', 'use', 'small', 'area', 'cooked', 'iced', 'whole', 'day.', 'mean', 'little', 'much', 'process.', 'remaning', 'warranted.', 'inside', 'sandwiches.', 'guacamole', 'open', 'through', 'about.', 'right', 'survived', 'store', 'anywhere', 'almost', 'juicy', 'do', 'mention', 'best', 'bit', 'farm', 'remaining', 'cluck-e-tos!', 'before', 'sub', 'kill', 'unimpressive.', 'hope', 'items', 'been.', 'loyal', 'very', 'delivered', 'my', 'place!!', 'treadmill.this', 'gravy....definitely', 'raising', '(i', 'here', 'rank', 'from', 'piece', 'too.', 'break.', 'charleston', '4.5,', 'food', 'however,', 'ate', 'nostalgic', 'eating', 'needed,', 'cute.', 'cut', 'left', 'lunch', 'tasted', 'better', 'tiny', 'us', 'meal', 'ive', 'grass', 'nor', '2', 'cluck', \"capriotti's\", 'quality\";', 'go', 'yelp', 'grimy', 'put', 'favorite', 'waiting', 'wheelchair', 'few', 'refried.', 'grossly', 'items.', '5.', 'say', 'kinds', 'hand', 'location', 'seasoning.', 'glad', 'recommend', 'by.', 'deep', 'every', 'leaving.', 'while', 'of', 'pepper', 'fast', 'also', 'would', 'charge', 'clucker', 'been', 'simply', 'day)', 'roll.', 'face', 'this', 'were', \"you'll\", 'fork', 'want', 'something,', 'many', 'way', \"cane's\", 'provided', 'dip', 'nearly', 'vile,', 'literally', 'remember', 'talking', 'bag', \"i'd\", 'they', 'these', 'even', 'can', 'tried', 'stepping', 'greasy!', 'eye', 'the', 'over', 'indeed', '&', 'fact,', 'salt', 'eyou', 'just', 'there', 'conclusion:', 'k', 'salt.', 'me.', 'your', 'sandwich', 'hyponatremia.', 'keep', 'amazing,', 'past', 'pressure', 'except', 'with', 'tos', 'time', 'responded.', 'pigsty', 'seeping', 'jesus', 'dinner', 'salsa.', 'be!', 'signature', 'calories', 'menu', 'suffering', 'corner', 'course,', 'crammed', 'taquitos', 'costco.', 'amazing', 'truth', 'hair-', 'coleslaw.', 'unabashed', 'anywhere.', 'an', 'not,', 'delicious,', 'thru', 'white', 'but', \"wasn't\", 'third', 'chicken.', 'lettuce', 'single-handedly', 'absolutely', 'several', \"they'd\", 'meat)', 'old', 'really', 'frozen', 'healthy', 'inflate', 'came', 'business,', 'account', 'con:', 'able', 'las', 'so', 'i', 'to', 'last', 'place', 'side', 'taste', 'never', 'always', 'perfectly,', 'lone', 'doorway', 'at', 'basket', 'super', 'drinking,', 'needing', 'like', \"won't\", 'opted', 'which', 'criminally', 'thing', 'grease', 'probably', 'drive', 'own.', 'part', 'opinion', 'asking', 'sandwiches', 'insects', \"don't\", 'one', 'diet', 'clucker).', 'quality', 'case', 'gobbler', 'incredible', 'months,', 'being', 'kfc', 'things', 'a', 'head', 'non-chain', 'stars', 'chicken', 'under', 'great!', 'his', 'each', 'decades.', 'who', 'used', '5', '30%', 'skinny', 'phone', '(white', 'up', 'risk!', 'made', '96-pack', 'knee', 'might', 'it', 'decided', 'gravy', 'closes.', 'by', 'weight.', 'tripled', 'appetizer.', 'womb', 'smelled', 'galore.', 'clucker)', 'worth', 'fried', 'no', 'attached', 'business', 'americanized', 'fries,', 'translation', 'accessible.', 'only', 'memories', 'neighborhood', 'spots', 'since', 'steak', 'joke,', 'better.', 'individual', 'friendly,', 'first', 'noises', 'thin', 'sh**', 'fries', 'felt', 'stop', \"(i'll\", 'love', 'everything', 'tender', 'said,', 'tagline', \"e'\", 'combined.', \"didn't\", 'is', 'less', 'vegas,', 'letter', 'them', 'we', 'mashed', 'affair.', 'well', 'basket!', 'am', 'their', 'eventually', 'delicious', 'cost...$8', 'too', 'hopper.', 'originals.classic', 'nice', 'good', 'salting', 'for', 'cheese,', 'when', 'believer', 'ask', 'items.not', 'had', 'everything.', 'well.', 'girl', 'around.', 'cluckeetos', 'two', 'next', 'pro:', 'amount', 'choice', 'various', 'usually', 'seriously', 'it.', 'shoved', 'coming', 'gobblers', 'down', 'music', 'chicken,', 'vegas', 'meat', 'associated', 'using', 'blood', 'rest', 'city!', 'succeeding', 'on', \"there's\", 'because', 'rating', 'coworkers,', 'neither', 'cluckers', 'embarrassing', 'own', '(minus', 'watching', '10', 'chains', 'special', 'lettuce,', 'crime!', 'chick-fil-a', '--', 'have', 'seasoned', 'owner', 'playing', 'potatoes', 'guacamole.', 'lunch,', 'extra', 'wrote', 'or', 'reasons.', 'complain', 'imagine', 'any', 'fairly', 'if', 'fries.', 'neighborhood.farm', 'but,', 'kick', 'has', 'our', 'itself', 'recycled', 'tea', 'less!', 'years,', 'small,', 'instead.', 'eat.', 'another', 'fried!', 'drove', 'homophobia', 'after', 'back', 'oh,', 'amazing.', 'potatoes,', 'experience', 'liked', 'mart', '(?)', 'as', 'state!!!', 'establishment', 'going', 'great.', 'walking', \"you're\", 'cluck-e-tos,', 'clientele.', 'dining', 'away', 'chose', 'you', 'ever', 'expect', 'it!', 'deals', 'perfectly.', 'prices,', 'not', 'carried', 'fast.', 'gravy,', 'high'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyze = vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(my', 'size', 'thursdays', 'unless', 'one,', 'tropicana', 'fat', 'heavily', 'those', 'sneak', 'out', 'cfa).', 'list', 'strong', 'over*priced', \"up'\", 'paid', 'was', 'and', 'make', 'minutes', 'slowed', '(great', 'me', 'night', 'staff', 'turkey', \"son's\", 'instead,', 'guythis', 'forgot', 'chicken--ever!get', 'w.', 'old.i', 'today,', 'joke.', 'instead', 'keeping', 'greasy', 'bones.', 'forget', 'steve', 'are', 'kid', 'knife', 'hands', 'same', 'fingers', 'served', 'terrible!', 'went', 'come', 'eat', 'kids', 'half', 'else', 'than', 'bobbie,', 'all', 'until', 'crunchy', 'get', 'any-type', 'into', \"'dem\", 'delights', 'cholesterol', 'did', 'already', 'had)', 'snacker', 'close', 'in', 'hahaa!', 'try', 'times', \"'grew\", 'heard', '\"we\\'re', 'themselves', 'way.', 'wash', '-', 'cleanest', 'rekindle', 'swill', 'order', 'reviewing,', 'sad', 'top', 'animal', 'kid.', \"he's\", 'world', 'told', 'more', '6', 'getting', 'french', 'food,', 'sure.', 'trade', 'anything', 'dilutional', \"i'm\", 'that', 'other', 'gravey,', 'not!!!', 'filthy!', 'restaurant', 'be', 'trying', 'wife', '$9', 'will', 'table.', 'use', 'small', 'area', 'cooked', 'iced', 'whole', 'day.', 'mean', 'little', 'much', 'process.', 'remaning', 'warranted.', 'inside', 'sandwiches.', 'guacamole', 'open', 'through', 'about.', 'right', 'survived', 'store', 'anywhere', 'almost', 'juicy', 'do', 'mention', 'best', 'bit', 'farm', 'remaining', 'cluck-e-tos!', 'before', 'sub', 'kill', 'unimpressive.', 'hope', 'items', 'been.', 'loyal', 'very', 'delivered', 'my', 'place!!', 'treadmill.this', 'gravy....definitely', 'raising', '(i', 'here', 'rank', 'from', 'piece', 'too.', 'break.', 'charleston', '4.5,', 'food', 'however,', 'ate', 'nostalgic', 'eating', 'needed,', 'cute.', 'cut', 'left', 'lunch', 'tasted', 'better', 'tiny', 'us', 'meal', 'ive', 'grass', 'nor', '2', 'cluck', \"capriotti's\", 'quality\";', 'go', 'yelp', 'grimy', 'put', 'favorite', 'waiting', 'wheelchair', 'few', 'refried.', 'grossly', 'items.', '5.', 'say', 'kinds', 'hand', 'location', 'seasoning.', 'glad', 'recommend', 'by.', 'deep', 'every', 'leaving.', 'while', 'of', 'pepper', 'fast', 'also', 'would', 'charge', 'clucker', 'been', 'simply', 'day)', 'roll.', 'face', 'this', 'were', \"you'll\", 'fork', 'want', 'something,', 'many', 'way', \"cane's\", 'provided', 'dip', 'nearly', 'vile,', 'literally', 'remember', 'talking', 'bag', \"i'd\", 'they', 'these', 'even', 'can', 'tried', 'stepping', 'greasy!', 'eye', 'the', 'over', 'indeed', '&', 'fact,', 'salt', 'eyou', 'just', 'there', 'conclusion:', 'k', 'salt.', 'me.', 'your', 'sandwich', 'hyponatremia.', 'keep', 'amazing,', 'past', 'pressure', 'except', 'with', 'tos', 'time', 'responded.', 'pigsty', 'seeping', 'jesus', 'dinner', 'salsa.', 'be!', 'signature', 'calories', 'menu', 'suffering', 'corner', 'course,', 'crammed', 'taquitos', 'costco.', 'amazing', 'truth', 'hair-', 'coleslaw.', 'unabashed', 'anywhere.', 'an', 'not,', 'delicious,', 'thru', 'white', 'but', \"wasn't\", 'third', 'chicken.', 'lettuce', 'single-handedly', 'absolutely', 'several', \"they'd\", 'meat)', 'old', 'really', 'frozen', 'healthy', 'inflate', 'came', 'business,', 'account', 'con:', 'able', 'las', 'so', 'i', 'to', 'last', 'place', 'side', 'taste', 'never', 'always', 'perfectly,', 'lone', 'doorway', 'at', 'basket', 'super', 'drinking,', 'needing', 'like', \"won't\", 'opted', 'which', 'criminally', 'thing', 'grease', 'probably', 'drive', 'own.', 'part', 'opinion', 'asking', 'sandwiches', 'insects', \"don't\", 'one', 'diet', 'clucker).', 'quality', 'case', 'gobbler', 'incredible', 'months,', 'being', 'kfc', 'things', 'a', 'head', 'non-chain', 'stars', 'chicken', 'under', 'great!', 'his', 'each', 'decades.', 'who', 'used', '5', '30%', 'skinny', 'phone', '(white', 'up', 'risk!', 'made', '96-pack', 'knee', 'might', 'it', 'decided', 'gravy', 'closes.', 'by', 'weight.', 'tripled', 'appetizer.', 'womb', 'smelled', 'galore.', 'clucker)', 'worth', 'fried', 'no', 'attached', 'business', 'americanized', 'fries,', 'translation', 'accessible.', 'only', 'memories', 'neighborhood', 'spots', 'since', 'steak', 'joke,', 'better.', 'individual', 'friendly,', 'first', 'noises', 'thin', 'sh**', 'fries', 'felt', 'stop', \"(i'll\", 'love', 'everything', 'tender', 'said,', 'tagline', \"e'\", 'combined.', \"didn't\", 'is', 'less', 'vegas,', 'letter', 'them', 'we', 'mashed', 'affair.', 'well', 'basket!', 'am', 'their', 'eventually', 'delicious', 'cost...$8', 'too', 'hopper.', 'originals.classic', 'nice', 'good', 'salting', 'for', 'cheese,', 'when', 'believer', 'ask', 'items.not', 'had', 'everything.', 'well.', 'girl', 'around.', 'cluckeetos', 'two', 'next', 'pro:', 'amount', 'choice', 'various', 'usually', 'seriously', 'it.', 'shoved', 'coming', 'gobblers', 'down', 'music', 'chicken,', 'vegas', 'meat', 'associated', 'using', 'blood', 'rest', 'city!', 'succeeding', 'on', \"there's\", 'because', 'rating', 'coworkers,', 'neither', 'cluckers', 'embarrassing', 'own', '(minus', 'watching', '10', 'chains', 'special', 'lettuce,', 'crime!', 'chick-fil-a', '--', 'have', 'seasoned', 'owner', 'playing', 'potatoes', 'guacamole.', 'lunch,', 'extra', 'wrote', 'or', 'reasons.', 'complain', 'imagine', 'any', 'fairly', 'if', 'fries.', 'neighborhood.farm', 'but,', 'kick', 'has', 'our', 'itself', 'recycled', 'tea', 'less!', 'years,', 'small,', 'instead.', 'eat.', 'another', 'fried!', 'drove', 'homophobia', 'after', 'back', 'oh,', 'amazing.', 'potatoes,', 'experience', 'liked', 'mart', '(?)', 'as', 'state!!!', 'establishment', 'going', 'great.', 'walking', \"you're\", 'cluck-e-tos,', 'clientele.', 'dining', 'away', 'chose', 'you', 'ever', 'expect', 'it!', 'deals', 'perfectly.', 'prices,', 'not', 'carried', 'fast.', 'gravy,', 'high']\n"
     ]
    }
   ],
   "source": [
    "feature_name = list(negVocab)\n",
    "print(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "\n",
    "posPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/large_movie_review_dataset/test/pos\"\n",
    "negPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/large_movie_review_dataset/test/neg\"\n",
    "os.chdir(negPath)\n",
    "filenames = os.listdir(negPath)\n",
    "allFiles = []\n",
    "for i in filenames:\n",
    "    if os.path.splitext(i)[1] == '.txt':\n",
    "        allFiles.append(i)\n",
    "print(len(allFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "f = open(os.path.join(os.getcwd(),\"newsample.txt\"),\"r\").readlines()\n",
    "f1 = []\n",
    "for k in f:\n",
    "    f1.append(better_Tokenizer(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "posPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/large_movie_review_dataset/test/pos\"\n",
    "# negPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/large_movie_review_dataset/test/neg\"\n",
    "os.chdir(posPath)\n",
    "filenames = os.listdir(posPath)\n",
    "allFiles = []\n",
    "for i in filenames:\n",
    "    if os.path.splitext(i)[1] == '.txt':\n",
    "        allFiles.append(i)\n",
    "print(len(allFiles))\n",
    "flag = True\n",
    "data = np.array([2,3,1,0])\n",
    "for f in allFiles:\n",
    "    if flag == True:\n",
    "        with open(os.path.join(posPath,f),'r',encoding='latin1') as doc:\n",
    "            words = []\n",
    "            content = better_Tokenizer(doc.read())\n",
    "            words.append(content)\n",
    "            data = vectorizer.transform(words).toarray()\n",
    "            flag = False\n",
    "    else:\n",
    "        with open(os.path.join(posPath,f),'r',encoding='latin1') as doc:\n",
    "            words = []\n",
    "            content = better_Tokenizer(doc.read())\n",
    "            words.append(content)\n",
    "            data = np.row_stack((data, vectorizer.transform(words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 47393)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "negPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/large_movie_review_dataset/test/neg\"\n",
    "# negPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/1\"\n",
    "os.chdir(negPath)\n",
    "filenames = os.listdir(negPath)\n",
    "allFiles = []\n",
    "for i in filenames:\n",
    "    if os.path.splitext(i)[1] == '.txt':\n",
    "        allFiles.append(i)\n",
    "print(len(allFiles))\n",
    "flag = True\n",
    "data1 = np.array([2,3,1,0])\n",
    "for f in allFiles:\n",
    "    if flag == True:\n",
    "        with open(os.path.join(negPath,f),'r',encoding='latin1') as doc:\n",
    "            words = []\n",
    "            content = better_Tokenizer(doc.read())\n",
    "            words.append(content)\n",
    "            data1 = vectorizer.transform(words).toarray()\n",
    "            flag = False\n",
    "    else:\n",
    "        with open(os.path.join(negPath,f),'r',encoding='latin1') as doc:\n",
    "            words = []\n",
    "            content = better_Tokenizer(doc.read())\n",
    "            words.append(content)\n",
    "            data1 = np.row_stack((data1, vectorizer.transform(words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226, 47393)\n"
     ]
    }
   ],
   "source": [
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data3 = np.row_stack((data,data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4063, 47393)\n"
     ]
    }
   ],
   "source": [
    "print(data3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/1\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('rowsdata.npy',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2058, 47393)\n"
     ]
    }
   ],
   "source": [
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2058, 47393)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data1 = np.load('rowsdata1.npy')\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2005, 47393)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data1 = np.load('rowsdata.npy')\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(data3[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['22', '33']\n"
     ]
    }
   ],
   "source": [
    "cbb = []\n",
    "cbb.append('22')\n",
    "cbb.append('33')\n",
    "print(cbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(1, 502)\n",
      "[[0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# data = []\n",
    "a1 = vectorizer.transform(['The US is going to substantially reduce taxes and regulations on businesses, but any business that leaves our country for another country, fires its employees, builds a new factory or plant in the other country, and then thinks it will sell its product back into the US without retribution or consequence, is WRONG! [sic], the Republican tweeted.']).toarray()\n",
    "a2 = vectorizer.transform(['This tax will make leaving financially difficult, but these companies are able to move between all 50 states, with no tax or tariff being charged.']).toarray()\n",
    "a3 = vectorizer.transform(['Experts have waned that the president-elect will face legal challenges if he tries to impose tariffs on specific companies without congressional approval.']).toarray()\n",
    "import numpy as np\n",
    "data = np.row_stack((a1,a2,a3))\n",
    "print(data)\n",
    "print(a1.shape)\n",
    "print(a2)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226, 22)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA   \n",
    "pca=PCA(n_components=22)  \n",
    "newData1=pca.fit_transform(data1)\n",
    "print(newData1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 22)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA   \n",
    "pca=PCA(n_components=22)  \n",
    "newData=pca.fit_transform(data)\n",
    "print(newData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.41545038e-01   1.14127336e-02   7.46331079e-03   5.48259937e-03\n",
      "   3.68668129e-03   2.36509739e-03   2.19242163e-03   1.91890767e-03\n",
      "   1.62242279e-03   1.39985387e-03   1.31623348e-03   1.24262477e-03\n",
      "   1.09963976e-03   1.02615411e-03   8.45348005e-04   8.16727285e-04\n",
      "   7.13206008e-04   6.60188489e-04   6.11398394e-04   5.92563469e-04\n",
      "   4.86933522e-04   4.71009366e-04]\n",
      "0.988971093331\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "sum1 = 0\n",
    "for k in pca.explained_variance_ratio_:\n",
    "    sum1 = sum1+k\n",
    "print(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 35)\n"
     ]
    }
   ],
   "source": [
    "print(newData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.42528935e-01   1.07923139e-02  -2.86834220e-14 ...,   1.11645451e-02\n",
      "    1.84855980e-01   0.00000000e+00]\n",
      " [  3.04834495e-01   2.02087333e-02  -1.12796596e-13 ...,   1.14104313e-02\n",
      "    1.77786431e-01   0.00000000e+00]\n",
      " [  4.37848988e-01   1.23286276e-01  -1.60694105e-13 ...,  -1.66414687e-03\n",
      "   -2.33467979e-02   0.00000000e+00]\n",
      " ..., \n",
      " [  5.75488874e-01   3.79477878e-02  -1.61309792e-13 ...,   5.06627486e-03\n",
      "    9.29556444e-02   0.00000000e+00]\n",
      " [  4.20871498e-02   1.02955793e-02  -3.08299450e-14 ...,  -7.37239518e-04\n",
      "    7.13692511e-03   0.00000000e+00]\n",
      " [  8.41614449e-02  -8.91536527e-03   2.30940299e-14 ...,   2.34619264e-03\n",
      "    5.23848814e-02   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "B=pca.inverse_transform(newData)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 2 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from liblinear import *\n",
    "prob = problem([1,-1], [{1:1, 2:1, 3:1}, {1:-1, 2:-1, 3:-1}])\n",
    "param = parameter('-s 6 -c 4')\n",
    "m = liblinear.train(prob, param)\n",
    "x0, max_idx = gen_feature_nodearray({1:-1, 2:-1, 3:1})\n",
    "label = liblinear.predict(m, x0)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "posPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/large_movie_review_dataset/train/pos\"\n",
    "negPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/large_movie_review_dataset/train/neg\"\n",
    "\n",
    "os.chdir(posPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/large_movie_review_dataset/train/pos\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9102\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "\n",
    "posPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/large_movie_review_dataset/train/pos\"\n",
    "negPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/large_movie_review_dataset/train/neg\"\n",
    "os.chdir(negPath)\n",
    "filenames = os.listdir(negPath)\n",
    "allFiles = []\n",
    "for i in filenames:\n",
    "    if os.path.splitext(i)[1] == '.txt':\n",
    "        allFiles.append(i)\n",
    "print(len(allFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2058\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "\n",
    "posPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/large_movie_review_dataset/train/pos\"\n",
    "negPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/1\"\n",
    "filenames = os.listdir(negPath)\n",
    "allFiles = []\n",
    "for i in filenames:\n",
    "    if os.path.splitext(i)[1] == '.txt':\n",
    "        allFiles.append(i)\n",
    "print(len(allFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/large_movie_review_dataset/train/pos\"\n",
    "negPath = \"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/1\"\n",
    "fwrite = open('/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/newsample.txt','a')\n",
    "for f in allFiles:\n",
    "    with open(os.path.join(negPath,f),'r',encoding='latin1') as doc:\n",
    "        content = doc.read()\n",
    "        fwrite.write(content)\n",
    "fwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "testsamples = []\n",
    "print(testsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451\n"
     ]
    }
   ],
   "source": [
    "print(len(testsamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n"
     ]
    }
   ],
   "source": [
    "print(len(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fwrite = open('/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/testtrainnew.txt','a')\n",
    "for k in data:\n",
    "    s = \"0\"\n",
    "    count = 1\n",
    "    for col in k:\n",
    "        s = s+\" \"+str(count)+\":\"+str(col)\n",
    "        count=count+1\n",
    "#     fwrite.write(s+\"\\n\")\n",
    "    testsamples.append(s)\n",
    "# fwrite.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fwrite = open('/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/fulltest.txt','a')\n",
    "for k in testsamples:\n",
    "    fwrite.write(k+\"\\n\")\n",
    "fwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4063\n"
     ]
    }
   ],
   "source": [
    "f2 = open('/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants/fulldata.txt','r').readlines()\n",
    "print(len(f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4063\n"
     ]
    }
   ],
   "source": [
    "print(len(f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s s\n"
     ]
    }
   ],
   "source": [
    "s = \"s\"+\" s\"\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1 \n",
      "0 \n",
      "1 \n",
      "1 \n",
      "0 \n",
      "1 \n",
      "1 \n",
      "1 \n",
      "1 \n",
      "1 \n",
      "0 \n",
      "1 \n",
      "0 \n",
      "0 \n",
      "0 \n",
      "1 \n",
      "0 \n",
      "1 \n",
      "1 \n"
     ]
    }
   ],
   "source": [
    "for k in f2[0:20]:\n",
    "    print(k[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/chenjiannan/Documents/svm/liblinear-2.1/python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/Users/chenjiannan/Documents/UMass/2016Fall/CS585-INLP/project/restaurants\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named liblinearutil",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8c946411142b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mliblinearutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_read_problem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testtrainnew.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-s 3 -c 0.000004'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named liblinearutil"
     ]
    }
   ],
   "source": [
    "\n",
    "from liblinearutil import *\n",
    "y, x = svm_read_problem('testtrainnew.txt')\n",
    "prob = problem(y, x)\n",
    "param = parameter('-s 3 -c 0.000004')\n",
    "# m = train(y, x, '-c 5')\n",
    "# m = train(prob, '-w1 5 -c 5')\n",
    "m = train(prob, param)\n",
    "CV_ACC = train(y, x, '-v 3')\n",
    "best_C, best_rate = train(y, x, '-C -s 0')\n",
    "y1, x1 = svm_read_problem('testtestnew.txt')\n",
    "p_labs, p_acc, p_vals = predict(y1, x1, m, '-q')\n",
    "print(p_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2335\n",
      "7530\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "f = open(os.path.join(os.getcwd(),\"a8a.t\"),\"r\").readlines()\n",
    "posCount = 0\n",
    "negCount = 0\n",
    "for k in f:\n",
    "    ss = k.split()\n",
    "    val = ss[0]\n",
    "    if val == '+1':\n",
    "        posCount=posCount+1\n",
    "    else:\n",
    "        negCount=negCount+1\n",
    "print(posCount)\n",
    "print(negCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2),token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bi grams', 'grams are', 'are cool']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze('Bi-grams are cool!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "ss = \"22 22\"\n",
    "print(ss[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
